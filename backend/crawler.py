import requests
from bs4 import BeautifulSoup
import random
import re  # å¼•å…¥æ­£åˆ™åº“ï¼Œç”¨æ¥å¤„ç†å¤šå…³é”®è¯

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
]

def fetch_news_ithome(target_count=10, keyword=None):
    url = "https://www.ithome.com/"
    headers = { "User-Agent": random.choice(USER_AGENTS) }

    print(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å– ITä¹‹å®¶ (ç›®æ ‡: {target_count}æ¡, åŸå§‹å…³é”®è¯: {keyword})...")
    
    # === 1. å¤„ç†å¤šå…³é”®è¯é€»è¾‘ ===
    keywords_list = []
    if keyword:
        # ä½¿ç”¨æ­£åˆ™æŒ‰ç…§ ç©ºæ ¼ã€ä¸­æ–‡é€—å·ã€è‹±æ–‡é€—å· è¿›è¡Œåˆ†å‰²
        # ä¾‹å¦‚è¾“å…¥ "å°ç±³, åä¸º" -> ['å°ç±³', 'åä¸º']
        keywords_list = re.split(r'[,\sï¼Œ]+', keyword.strip())
        # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²
        keywords_list = [k for k in keywords_list if k]
        print(f"ğŸ” è§£æåçš„æœç´¢è¯åˆ—è¡¨: {keywords_list}")

    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, "html.parser")
        
        candidates = []
        items = soup.select(".nl > li")
        
        for item in items:
            link_tag = item.find("a")
            if link_tag:
                title = link_tag.get_text(strip=True)
                href = link_tag.get("href")
                
                if href and "ithome.com" in href:
                    # === 2. æ ¸å¿ƒä¿®æ”¹ï¼šæ”¯æŒä»»æ„åŒ¹é… ===
                    if keywords_list:
                        # é€»è¾‘ï¼šå¦‚æœæ ‡é¢˜é‡ŒåŒ…å«åˆ—è¡¨ä¸­çš„ã€ä»»æ„ä¸€ä¸ªã€‘è¯ï¼Œå°±ç®—åŒ¹é…æˆåŠŸ
                        # any() å‡½æ•°ï¼šåªè¦æœ‰ä¸€ä¸ªæ˜¯ Trueï¼Œå°±è¿”å› True
                        is_match = any(k in title for k in keywords_list)
                        if not is_match:
                            continue # éƒ½ä¸åŒ…å«ï¼Œè·³è¿‡
                    
                    candidates.append({"title": title, "link": href})
                    
                    if len(candidates) >= target_count:
                        break
        
        print(f"âœ… ç­›é€‰å‡º {len(candidates)} æ¡æœ‰æ•ˆæ–°é—»")
        return candidates

    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        return []

def fetch_article_content(url):
    headers = { "User-Agent": random.choice(USER_AGENTS) }
    try:
        resp = requests.get(url, headers=headers, timeout=5)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, "html.parser")
        content_div = soup.find("div", id="paragraph")
        if content_div:
            return content_div.get_text(strip=True)
        return ""
    except:
        return ""